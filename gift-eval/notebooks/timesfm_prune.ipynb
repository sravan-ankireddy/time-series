{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Running timesfm models on gift-eval benchmark\n",
    "\n",
    "This notebook shows how to run timesfm-2.0 on gift-eval.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install timesfm package in a python 3.10.x env:\n",
    "``\n",
    "pip install timesfm[pax]\n",
    "``\n",
    "\n",
    "You can also try the torch version in a python 3.11.x env:\n",
    "``\n",
    "pip install timesfm[torch]\n",
    "``\n",
    "\n",
    "After that you can install the gift-eval package:\n",
    "``\n",
    "pip install -e .\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825107819c0e422e8e4bc2a10e8c55b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: bizitobs_l2c/H (1 of 2)\n",
      "Processing dataset: m4_weekly (2 of 2)\n",
      "Processing entry: (13, 'm4_weekly/W/short', 'm4_weekly', False)\n",
      "Dataset size: 359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef54e46f3acd461891a8c539ade40765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [00:00, 1120.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for m4_weekly have been written to ../results_prune/timesfm_2_0_500m/all_results.csv\n",
      "Processing entry: (48, 'bizitobs_l2c/H/short', 'bizitobs_l2c/H', True)\n",
      "Dataset size: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4df539beea490d988d81b89d86e00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:00, 990.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/H have been written to ../results_prune/timesfm_2_0_500m/all_results.csv\n",
      "Processing entry: (480, 'bizitobs_l2c/H/medium', 'bizitobs_l2c/H', True)\n",
      "Dataset size: 7\n",
      "Jitting for new prediction length.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974269dd3c5445c6ad4a6d16c7173632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 604.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/H have been written to ../results_prune/timesfm_2_0_500m/all_results.csv\n",
      "Processing entry: (720, 'bizitobs_l2c/H/long', 'bizitobs_l2c/H', True)\n",
      "Dataset size: 7\n",
      "Jitting for new prediction length.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c29ba022f14759b8a83c0a234043e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 598.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/H have been written to ../results_prune/timesfm_2_0_500m/all_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>eval_metrics/MSE[mean]</th>\n",
       "      <th>eval_metrics/MSE[0.5]</th>\n",
       "      <th>eval_metrics/MAE[0.5]</th>\n",
       "      <th>eval_metrics/MASE[0.5]</th>\n",
       "      <th>eval_metrics/MAPE[0.5]</th>\n",
       "      <th>eval_metrics/sMAPE[0.5]</th>\n",
       "      <th>eval_metrics/MSIS</th>\n",
       "      <th>eval_metrics/RMSE[mean]</th>\n",
       "      <th>eval_metrics/NRMSE[mean]</th>\n",
       "      <th>eval_metrics/ND[0.5]</th>\n",
       "      <th>eval_metrics/mean_weighted_sum_quantile_loss</th>\n",
       "      <th>domain</th>\n",
       "      <th>num_variates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m4_weekly/W/short</td>\n",
       "      <td>timesfm_2_0_500m</td>\n",
       "      <td>347133.092095</td>\n",
       "      <td>347133.092095</td>\n",
       "      <td>283.604670</td>\n",
       "      <td>2.227370</td>\n",
       "      <td>0.067725</td>\n",
       "      <td>0.070797</td>\n",
       "      <td>19.264331</td>\n",
       "      <td>589.180017</td>\n",
       "      <td>0.107339</td>\n",
       "      <td>0.051668</td>\n",
       "      <td>0.041556</td>\n",
       "      <td>Econ/Fin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bizitobs_l2c/H/short</td>\n",
       "      <td>timesfm_2_0_500m</td>\n",
       "      <td>182.360313</td>\n",
       "      <td>182.360313</td>\n",
       "      <td>7.984540</td>\n",
       "      <td>0.778792</td>\n",
       "      <td>0.584974</td>\n",
       "      <td>0.751160</td>\n",
       "      <td>5.318400</td>\n",
       "      <td>13.504085</td>\n",
       "      <td>0.727905</td>\n",
       "      <td>0.430387</td>\n",
       "      <td>0.344183</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bizitobs_l2c/H/medium</td>\n",
       "      <td>timesfm_2_0_500m</td>\n",
       "      <td>284.471012</td>\n",
       "      <td>284.471012</td>\n",
       "      <td>12.325005</td>\n",
       "      <td>1.184435</td>\n",
       "      <td>1.329619</td>\n",
       "      <td>1.003966</td>\n",
       "      <td>17.404700</td>\n",
       "      <td>16.866268</td>\n",
       "      <td>1.021278</td>\n",
       "      <td>0.746298</td>\n",
       "      <td>0.640652</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bizitobs_l2c/H/long</td>\n",
       "      <td>timesfm_2_0_500m</td>\n",
       "      <td>336.421700</td>\n",
       "      <td>336.421700</td>\n",
       "      <td>13.225690</td>\n",
       "      <td>1.287402</td>\n",
       "      <td>1.566774</td>\n",
       "      <td>1.043593</td>\n",
       "      <td>24.820240</td>\n",
       "      <td>18.341802</td>\n",
       "      <td>1.120370</td>\n",
       "      <td>0.807863</td>\n",
       "      <td>0.716002</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dataset             model  eval_metrics/MSE[mean]   \n",
       "0      m4_weekly/W/short  timesfm_2_0_500m           347133.092095  \\\n",
       "1   bizitobs_l2c/H/short  timesfm_2_0_500m              182.360313   \n",
       "2  bizitobs_l2c/H/medium  timesfm_2_0_500m              284.471012   \n",
       "3    bizitobs_l2c/H/long  timesfm_2_0_500m              336.421700   \n",
       "\n",
       "   eval_metrics/MSE[0.5]  eval_metrics/MAE[0.5]  eval_metrics/MASE[0.5]   \n",
       "0          347133.092095             283.604670                2.227370  \\\n",
       "1             182.360313               7.984540                0.778792   \n",
       "2             284.471012              12.325005                1.184435   \n",
       "3             336.421700              13.225690                1.287402   \n",
       "\n",
       "   eval_metrics/MAPE[0.5]  eval_metrics/sMAPE[0.5]  eval_metrics/MSIS   \n",
       "0                0.067725                 0.070797          19.264331  \\\n",
       "1                0.584974                 0.751160           5.318400   \n",
       "2                1.329619                 1.003966          17.404700   \n",
       "3                1.566774                 1.043593          24.820240   \n",
       "\n",
       "   eval_metrics/RMSE[mean]  eval_metrics/NRMSE[mean]  eval_metrics/ND[0.5]   \n",
       "0               589.180017                  0.107339              0.051668  \\\n",
       "1                13.504085                  0.727905              0.430387   \n",
       "2                16.866268                  1.021278              0.746298   \n",
       "3                18.341802                  1.120370              0.807863   \n",
       "\n",
       "   eval_metrics/mean_weighted_sum_quantile_loss        domain  num_variates  \n",
       "0                                      0.041556      Econ/Fin             1  \n",
       "1                                      0.344183  Web/CloudOps             7  \n",
       "2                                      0.640652  Web/CloudOps             7  \n",
       "3                                      0.716002  Web/CloudOps             7  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "# short_datasets = \"m4_weekly\"\n",
    "\n",
    "med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "# med_long_datasets = \"bizitobs_l2c/H\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = list(set(short_datasets.split() + med_long_datasets.split()))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))\n",
    "\n",
    "\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]\n",
    "\n",
    "import timesfm\n",
    "\n",
    "# tfm = timesfm.TimesFm(\n",
    "#     hparams=timesfm.TimesFmHparams(\n",
    "#         backend=\"gpu\",\n",
    "#         per_core_batch_size=32,\n",
    "#         num_layers=50,\n",
    "#         horizon_len=128,\n",
    "#         context_len=2048,\n",
    "#         use_positional_embedding=False,\n",
    "#         output_patch_len=128,\n",
    "#     ),\n",
    "#     checkpoint=timesfm.TimesFmCheckpoint(\n",
    "#         huggingface_repo_id=\"google/timesfm-2.0-500m-jax\"),\n",
    "# )\n",
    "\n",
    "# If you are using the pytorch version:\n",
    "tfm = timesfm.TimesFm(\n",
    "    hparams=timesfm.TimesFmHparams(\n",
    "        backend=\"gpu\",\n",
    "        per_core_batch_size=32,\n",
    "        num_layers=50,\n",
    "        horizon_len=128,\n",
    "        context_len=2048,\n",
    "        use_positional_embedding=False,\n",
    "        output_patch_len=128,\n",
    "    ),\n",
    "    checkpoint=timesfm.TimesFmCheckpoint(\n",
    "        huggingface_repo_id=\"google/timesfm-2.0-500m-pytorch\"),\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast\n",
    "\n",
    "\n",
    "class TimesFmPredictor:\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      tfm,\n",
    "      prediction_length: int,\n",
    "      ds_freq: str,\n",
    "      *args,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    self.tfm = tfm\n",
    "    self.prediction_length = prediction_length\n",
    "    if self.prediction_length > self.tfm.horizon_len:\n",
    "      self.tfm.horizon_len = (\n",
    "          (self.prediction_length + self.tfm.output_patch_len - 1) //\n",
    "          self.tfm.output_patch_len) * self.tfm.output_patch_len\n",
    "      print('Jitting for new prediction length.')\n",
    "    self.freq = timesfm.freq_map(ds_freq)\n",
    "\n",
    "  def predict(self, test_data_input, batch_size: int = 1024) -> List[Forecast]:\n",
    "    forecast_outputs = []\n",
    "    for batch in tqdm(batcher(test_data_input, batch_size=batch_size)):\n",
    "      context = []\n",
    "      for entry in batch:\n",
    "        arr = np.array(entry[\"target\"])\n",
    "        context.append(arr)\n",
    "      freqs = [self.freq] * len(context)\n",
    "      _, full_preds = self.tfm.forecast(context, freqs, normalize=True)\n",
    "      full_preds = full_preds[:, 0:self.prediction_length, 1:]\n",
    "      forecast_outputs.append(full_preds.transpose((0, 2, 1)))\n",
    "    forecast_outputs = np.concatenate(forecast_outputs)\n",
    "\n",
    "    # Convert forecast samples into gluonts Forecast objects\n",
    "    forecasts = []\n",
    "    for item, ts in zip(forecast_outputs, test_data_input):\n",
    "      forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "      forecasts.append(\n",
    "          QuantileForecast(\n",
    "              forecast_arrays=item,\n",
    "              forecast_keys=list(map(str, self.tfm.quantiles)),\n",
    "              start_date=forecast_start_date,\n",
    "          ))\n",
    "\n",
    "    return forecasts\n",
    "  \n",
    "import logging\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "all_ds_tuples = []\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "  ds_key = ds_name.split(\"/\")[0]\n",
    "  print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "  terms = [\"short\", \"medium\", \"long\"]\n",
    "  for term in terms:\n",
    "    if (term == \"medium\" or\n",
    "        term == \"long\") and ds_name not in med_long_datasets.split():\n",
    "      continue\n",
    "\n",
    "    if \"/\" in ds_name:\n",
    "      ds_key = ds_name.split(\"/\")[0]\n",
    "      ds_freq = ds_name.split(\"/\")[1]\n",
    "      ds_key = ds_key.lower()\n",
    "      ds_key = pretty_names.get(ds_key, ds_key)\n",
    "    else:\n",
    "      ds_key = ds_name.lower()\n",
    "      ds_key = pretty_names.get(ds_key, ds_key)\n",
    "      ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "    ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "    # Initialize the dataset\n",
    "    to_univariate = (False if Dataset(\n",
    "        name=ds_name, term=term, to_univariate=False).target_dim == 1 else True)\n",
    "    dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "    all_ds_tuples.append(\n",
    "        (dataset.prediction_length, ds_config, ds_name, to_univariate))\n",
    "    \n",
    "all_ds_tuples = sorted(all_ds_tuples)\n",
    "all_ds_tuples[0:10]\n",
    "\n",
    "\n",
    "model_name = \"timesfm_2_0_500m\"\n",
    "output_dir = f\"../results_prune/{model_name}\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "\n",
    "  # Write the header\n",
    "  writer.writerow([\n",
    "      \"dataset\",\n",
    "      \"model\",\n",
    "      \"eval_metrics/MSE[mean]\",\n",
    "      \"eval_metrics/MSE[0.5]\",\n",
    "      \"eval_metrics/MAE[0.5]\",\n",
    "      \"eval_metrics/MASE[0.5]\",\n",
    "      \"eval_metrics/MAPE[0.5]\",\n",
    "      \"eval_metrics/sMAPE[0.5]\",\n",
    "      \"eval_metrics/MSIS\",\n",
    "      \"eval_metrics/RMSE[mean]\",\n",
    "      \"eval_metrics/NRMSE[mean]\",\n",
    "      \"eval_metrics/ND[0.5]\",\n",
    "      \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "      \"domain\",\n",
    "      \"num_variates\",\n",
    "  ])\n",
    "\n",
    "for entry in all_ds_tuples:\n",
    "  prediction_length = entry[0]\n",
    "  ds_name = entry[2]\n",
    "  to_univariate = entry[3]\n",
    "  ds_config = entry[1]\n",
    "  ds_key, ds_freq, term = ds_config.split(\"/\")\n",
    "  dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "  season_length = get_seasonality(dataset.freq)\n",
    "  print(f\"Processing entry: {entry}\")\n",
    "  print(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "  predictor = TimesFmPredictor(\n",
    "      tfm=tfm,\n",
    "      prediction_length=dataset.prediction_length,\n",
    "      ds_freq=ds_freq,\n",
    "  )\n",
    "  # Measure the time taken for evaluation\n",
    "  res = evaluate_model(\n",
    "      predictor,\n",
    "      test_data=dataset.test_data,\n",
    "      metrics=metrics,\n",
    "      batch_size=1024,\n",
    "      axis=None,\n",
    "      mask_invalid_label=True,\n",
    "      allow_nan_forecast=False,\n",
    "      seasonality=season_length,\n",
    "  )\n",
    "\n",
    "  # Append the results to the CSV file\n",
    "  with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\n",
    "        ds_config,\n",
    "        model_name,\n",
    "        res[\"MSE[mean]\"][0],\n",
    "        res[\"MSE[0.5]\"][0],\n",
    "        res[\"MAE[0.5]\"][0],\n",
    "        res[\"MASE[0.5]\"][0],\n",
    "        res[\"MAPE[0.5]\"][0],\n",
    "        res[\"sMAPE[0.5]\"][0],\n",
    "        res[\"MSIS\"][0],\n",
    "        res[\"RMSE[mean]\"][0],\n",
    "        res[\"NRMSE[mean]\"][0],\n",
    "        res[\"ND[0.5]\"][0],\n",
    "        res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "        dataset_properties_map[ds_key][\"domain\"],\n",
    "        dataset_properties_map[ds_key][\"num_variates\"],\n",
    "    ])\n",
    "\n",
    "  print(f\"Results for {ds_name} have been written to {csv_file_path}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"../results_prune/{model_name}/all_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
