{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133b57f9-ba31-4292-919d-4ec2a8292210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Forecasting with YingLong Model\n",
    "\n",
    "## 1. Setup and Imports\n",
    "\n",
    "#First, we'll install any necessary packages and import all required libraries.\n",
    "# Install required packages (uncomment if not already installed)\n",
    "# !pip install transformers gluonts einops pandas matplotlib python-dotenv\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM\n",
    "from dotenv import load_dotenv\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast, SampleForecast\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "from gift_eval.data import Dataset\n",
    "import multiprocessing\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951e588-85b3-4eb7-864e-7217f0fb76ef",
   "metadata": {},
   "source": [
    "2. Configuration and Classes\n",
    "Define necessary configurations, logging filters, and the YingLongPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "270b749f-aff3-44d2-bd02-f99222563be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义日志过滤器以抑制特定的警告信息\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")\n",
    "\n",
    "# 定义模型配置\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    quantile_levels: Optional[List[float]] = None\n",
    "    forecast_keys: List[str] = field(init=False)\n",
    "    statsforecast_keys: List[str] = field(init=False)\n",
    "    intervals: Optional[List[int]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.forecast_keys = [\"mean\"]\n",
    "        self.statsforecast_keys = [\"mean\"]\n",
    "        if self.quantile_levels is None:\n",
    "            self.intervals = None\n",
    "            return\n",
    "\n",
    "        intervals = set()\n",
    "\n",
    "        for quantile_level in self.quantile_levels:\n",
    "            interval = round(200 * (max(quantile_level, 1 - quantile_level) - 0.5))\n",
    "            intervals.add(interval)\n",
    "            side = \"hi\" if quantile_level > 0.5 else \"lo\"\n",
    "            self.forecast_keys.append(str(quantile_level))\n",
    "            self.statsforecast_keys.append(f\"{side}-{interval}\")\n",
    "\n",
    "        self.intervals = sorted(intervals)\n",
    "\n",
    "# 定义 YingLongPredictor 类\n",
    "class YingLongPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        prediction_length: int,\n",
    "        num_samples=20,\n",
    "        future_token=4096,\n",
    "    ):\n",
    "        print(\"prediction_length:\", prediction_length)\n",
    "        self.prediction_length = prediction_length\n",
    "        self.num_samples = num_samples\n",
    "        self.model = model\n",
    "        self.future_token = future_token\n",
    "\n",
    "    def model_predict(\n",
    "        self, \n",
    "        context,\n",
    "        prediction_length,\n",
    "        future_token,\n",
    "        scaling=400,\n",
    "        max_length=4096*16,\n",
    "        *args, **predict_kwargs\n",
    "    ):\n",
    "        context = [\n",
    "            torch.nan_to_num(\n",
    "                x[-max_length:].to(gpu_device),\n",
    "                nan=torch.nanmean(x[-max_length:].to(gpu_device))\n",
    "            ) for x in context\n",
    "        ]\n",
    "\n",
    "        length = max([len(x) for x in context])\n",
    "        context = [\n",
    "            x[-length:] if len(x) >= length else torch.cat(\n",
    "                (\n",
    "                    torch.ones(length - x.shape[-1]).to(x.device) * torch.mean(x),\n",
    "                    x\n",
    "                )\n",
    "            ) for x in context\n",
    "        ]\n",
    "        x = torch.stack(context, dim=0)\n",
    "\n",
    "        scale_factor = 1\n",
    "        with torch.no_grad():\n",
    "            B, _ = x.shape\n",
    "            logits = 0\n",
    "            historys = [512, 1024, 2048, 4096]\n",
    "            if future_token < 1000:\n",
    "                future_token = (prediction_length // 32 + 1) * 32\n",
    "\n",
    "            used = 0\n",
    "            for history in historys:\n",
    "                if used == 0 or history <= x.shape[-1]:\n",
    "                    used += 2\n",
    "                else:\n",
    "                    continue\n",
    "                x_train = torch.cat((x.bfloat16(), -x.bfloat16()), dim=0)\n",
    "                x_train = x_train[..., -history:].bfloat16()\n",
    "\n",
    "                if x_train.shape[-1] % self.model.patch_size != 0:\n",
    "                    shape = (\n",
    "                        x_train.shape[0],\n",
    "                        self.model.patch_size - x.shape[-1] % self.model.patch_size\n",
    "                    )\n",
    "                    x_train = torch.cat(\n",
    "                        (\n",
    "                            torch.ones(shape).to(x_train.device) * x_train.mean(dim=-1, keepdim=True),\n",
    "                            x_train\n",
    "                        ),\n",
    "                        dim=-1\n",
    "                    )\n",
    "                    x_train = x_train.bfloat16()\n",
    "\n",
    "                logits_all,_ = self.model(idx=x_train, future_token=future_token)\n",
    "                logits_all = rearrange(logits_all, '(t b) l c d -> b (l c) d t', t=2)\n",
    "                logits += logits_all[..., 0] - logits_all[..., 1].flip(dims=[-1])\n",
    "\n",
    "            logits = logits / used\n",
    "            sampleHolder = rearrange(logits, 'b l c -> b c l').float().contiguous().cpu().detach()[:, :, :prediction_length]\n",
    "            return torch.nan_to_num(sampleHolder)\n",
    "\n",
    "    def predict(self, test_data_input, batch_size: int = 1024) -> List[Forecast]:\n",
    "        predict_kwargs = {\"num_samples\": self.num_samples}\n",
    "        while True:\n",
    "            try:\n",
    "                forecast_outputs = []\n",
    "                for batch in tqdm(batcher(test_data_input, batch_size=batch_size)):\n",
    "                    context = [torch.tensor(entry[\"target\"]) for entry in batch]\n",
    "                    forecast_outputs.append(\n",
    "                        self.model_predict(\n",
    "                            context,\n",
    "                            prediction_length=self.prediction_length,\n",
    "                            future_token=self.future_token,\n",
    "                            **predict_kwargs,\n",
    "                        ).numpy()\n",
    "                    )\n",
    "                forecast_outputs = np.concatenate(forecast_outputs)\n",
    "                break\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(\n",
    "                    f\"OutOfMemoryError at batch_size {batch_size}, reducing to {batch_size // 2}\"\n",
    "                )\n",
    "                batch_size //= 2\n",
    "\n",
    "        forecasts = []\n",
    "        for item, ts in zip(forecast_outputs, test_data_input):\n",
    "            forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "            forecasts.append(SampleForecast(samples=item, start_date=forecast_start_date))\n",
    "\n",
    "        return forecasts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef80da-476a-4599-ab62-4e6d8ad0e344",
   "metadata": {},
   "source": [
    "\n",
    "3. Prediction and Evaluation Functions\n",
    "Implement the core prediction logic and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a40f6b-5d5e-4f44-9c61-6e0ca20a6f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "gpu_device = device\n",
    "def run_task(\n",
    "    batch_size: int,\n",
    "    model,\n",
    "    future_token: int,\n",
    "    short_tasks: List[str],\n",
    "    long_tasks: List[str],\n",
    "    output_dir: str,\n",
    "    dataset_properties_map: dict,\n",
    "    model_name: str\n",
    "):\n",
    "    # 实例化评估指标\n",
    "    metrics = [\n",
    "        MSE(forecast_type=\"mean\"),\n",
    "        MSE(forecast_type=0.5),\n",
    "        MAE(),\n",
    "        MASE(),\n",
    "        MAPE(),\n",
    "        SMAPE(),\n",
    "        MSIS(),\n",
    "        RMSE(),\n",
    "        NRMSE(),\n",
    "        ND(),\n",
    "        MeanWeightedSumQuantileLoss(\n",
    "            quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # 将模型移动到指定设备并设为评估模式\n",
    "    model = model.to(device).bfloat16()\n",
    "    model.eval()\n",
    "\n",
    "    # 构建模型输出目录\n",
    "    model_name_suffix = f\"{model_name.split('/')[-1]}-{future_token}-4096\"\n",
    "    model_output_dir = os.path.join(output_dir, model_name_suffix)\n",
    "    if not os.path.isdir(model_output_dir):\n",
    "        os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    # 定义 CSV 文件路径\n",
    "    csv_file_path = os.path.join(model_output_dir, \"all_results.csv\")\n",
    "\n",
    "    # 美化名称映射\n",
    "    pretty_names = {\n",
    "        \"saugeenday\": \"saugeen\",\n",
    "        \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "        \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "        \"car_parts_with_missing\": \"car_parts\",\n",
    "    }\n",
    "\n",
    "    # 如果 CSV 文件不存在，创建并写入表头\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    \"dataset\",\n",
    "                    \"model\",\n",
    "                    \"eval_metrics/MSE[mean]\",\n",
    "                    \"eval_metrics/MSE[0.5]\",\n",
    "                    \"eval_metrics/MAE[0.5]\",\n",
    "                    \"eval_metrics/MASE[0.5]\",\n",
    "                    \"eval_metrics/MAPE[0.5]\",\n",
    "                    \"eval_metrics/sMAPE[0.5]\",\n",
    "                    \"eval_metrics/MSIS\",\n",
    "                    \"eval_metrics/RMSE[mean]\",\n",
    "                    \"eval_metrics/NRMSE[mean]\",\n",
    "                    \"eval_metrics/ND[0.5]\",\n",
    "                    \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "                    \"domain\",\n",
    "                    \"num_variates\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    # 合并短期任务和长期任务\n",
    "    all_datasets = list(set(short_tasks + long_tasks))\n",
    "    \n",
    "    for ds_num, ds_name in enumerate(all_datasets):\n",
    "        ds_key = ds_name.split(\"/\")[0]\n",
    "        print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        for term in terms:\n",
    "            if (term in [\"medium\", \"long\"]) and (ds_name not in long_tasks):\n",
    "                continue\n",
    "\n",
    "            if \"/\" in ds_name:\n",
    "                ds_key, ds_freq = ds_name.split(\"/\")\n",
    "                ds_key = ds_key.lower()\n",
    "                ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            else:\n",
    "                ds_key = ds_name.lower()\n",
    "                ds_key = pretty_names.get(ds_key, ds_key)\n",
    "                ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "            ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "            print(ds_config)\n",
    "            to_univariate = (\n",
    "                False\n",
    "                if Dataset(name=ds_name, term=term, to_univariate=False).target_dim == 1\n",
    "                else True\n",
    "            )\n",
    "            dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "            season_length = get_seasonality(dataset.freq)\n",
    "            print(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "            predictor = YingLongPredictor(\n",
    "                model=model,\n",
    "                prediction_length=dataset.prediction_length,\n",
    "                future_token=future_token,\n",
    "            )\n",
    "            # 执行模型评估\n",
    "            res = evaluate_model(\n",
    "                predictor,\n",
    "                test_data=dataset.test_data,\n",
    "                metrics=metrics,\n",
    "                batch_size=batch_size,\n",
    "                axis=None,\n",
    "                mask_invalid_label=True,\n",
    "                allow_nan_forecast=False,\n",
    "                seasonality=season_length,\n",
    "            )\n",
    "\n",
    "            # 将结果追加到 CSV 文件\n",
    "            with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(\n",
    "                    [\n",
    "                        ds_config,\n",
    "                        model_name_suffix,\n",
    "                        res[\"MSE[mean]\"][0],\n",
    "                        res[\"MSE[0.5]\"][0],\n",
    "                        res[\"MAE[0.5]\"][0],\n",
    "                        res[\"MASE[0.5]\"][0],\n",
    "                        res[\"MAPE[0.5]\"][0],\n",
    "                        res[\"sMAPE[0.5]\"][0],\n",
    "                        res[\"MSIS\"][0],\n",
    "                        res[\"RMSE[mean]\"][0],\n",
    "                        res[\"NRMSE[mean]\"][0],\n",
    "                        res[\"ND[0.5]\"][0],\n",
    "                        res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                        dataset_properties_map[ds_key][\"domain\"],\n",
    "                        dataset_properties_map[ds_key][\"num_variates\"],\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            print(f\"Results for {ds_name} have been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a3df3-9fd5-4841-9eb1-8ca1b45ce154",
   "metadata": {},
   "source": [
    "4. Dataset and Metrics Initialization\n",
    "Load environment variables, dataset properties, and prepare the list of datasets to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbf8198a-5a89-476b-a06f-d4086f00edcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 定义模型和输出配置\n",
    "model_name = \"qcw2333/YingLong_300m\"\n",
    "future_token = 4096\n",
    "output_dir = \"results_hf_0\"\n",
    "\n",
    "# 加载模型\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = model.to(device).bfloat16()\n",
    "#model.eval()\n",
    "\n",
    "# 加载数据集属性\n",
    "with open(\"dataset_properties.json\", \"r\") as f:\n",
    "    dataset_properties_map = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec9080-1ce9-4dc9-ae9e-98fd5d1254de",
   "metadata": {},
   "source": [
    "5. Multiprocessing Setup\n",
    "Define a function to handle predictions on a single GPU and utilize multiprocessing to run tasks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfcc8b21-76aa-40c4-b07a-31d34d3dac64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义任务列表，模仿 run-hf-4.sh 的四个并行任务\n",
    "tasks = [\n",
    "    {\n",
    "        \"batch_size\": 1024,\n",
    "        \"short_tasks\": [\n",
    "            \"temperature_rain_with_missing\",\n",
    "            \"m4_yearly\",\n",
    "            \"electricity/D\",\n",
    "            \"restaurant\",\n",
    "            \"kdd_cup_2018_with_missing/D\",\n",
    "            \"covid_deaths\",\n",
    "            \"M_DENSE/D\",\n",
    "            \"jena_weather/D\",\n",
    "            \"saugeenday/D\",\n",
    "            \"saugeenday/W\",\n",
    "            \"m4_monthly\"\n",
    "        ],\n",
    "        \"long_tasks\": [\n",
    "            \"LOOP_SEATTLE/5T\",\n",
    "            \"kdd_cup_2018_with_missing/H\",\n",
    "            \"SZ_TAXI/15T\",\n",
    "            \"ett1/15T\",\n",
    "            \"bizitobs_l2c/5T\",\n",
    "            \"bizitobs_l2c/H\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 1024,\n",
    "        \"short_tasks\": [\n",
    "            \"bitbrains_fast_storage/H\",\n",
    "            \"m4_daily\",\n",
    "            \"electricity/W\",\n",
    "            \"hierarchical_sales/W\",\n",
    "            \"m4_weekly\",\n",
    "            \"ett2/W\",\n",
    "            \"us_births/W\",\n",
    "            \"us_births/M\"\n",
    "        ],\n",
    "        \"long_tasks\": [\n",
    "            \"bitbrains_rnd/5T\",\n",
    "            \"electricity/H\",\n",
    "            \"bizitobs_service\",\n",
    "            \"jena_weather/H\",\n",
    "            \"ett2/15T\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 1024,\n",
    "        \"short_tasks\": [\n",
    "            \"hospital\",\n",
    "            \"LOOP_SEATTLE/D\",\n",
    "            \"m4_hourly\",\n",
    "            \"ett1/D\",\n",
    "            \"ett2/D\",\n",
    "            \"ett1/W\",\n",
    "            \"saugeenday/M\"\n",
    "        ],\n",
    "        \"long_tasks\": [\n",
    "            \"bitbrains_fast_storage/5T\",\n",
    "            \"solar/10T\",\n",
    "            \"M_DENSE/H\",\n",
    "            \"ett1/H\",\n",
    "            \"bizitobs_application\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 32,\n",
    "        \"short_tasks\": [\"car_parts_with_missing\"],\n",
    "        \"long_tasks\": []\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d0f8d3b-e6ac-4aee-b13d-9e202f85c5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Task 1 ===\n",
      "Processing dataset: LOOP_SEATTLE/5T (1 of 17)\n",
      "loop_seattle/5T/short\n",
      "Dataset size: 6460\n",
      "prediction_length: 48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d347c96e04942cbbd272a5e8884c2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6460it [02:37, 41.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LOOP_SEATTLE/5T have been written to results_hf_0/YingLong_300m-4096-4096/all_results.csv\n",
      "loop_seattle/5T/medium\n",
      "Dataset size: 6460\n",
      "prediction_length: 480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc1beff3c434110b0bbc12f22590908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tasks, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Running Task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mrun_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuture_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshort_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshort_tasks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlong_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlong_tasks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_properties_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_properties_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 110\u001b[0m, in \u001b[0;36mrun_task\u001b[0;34m(batch_size, model, future_token, short_tasks, long_tasks, output_dir, dataset_properties_map, model_name)\u001b[0m\n\u001b[1;32m    104\u001b[0m predictor \u001b[38;5;241m=\u001b[39m YingLongPredictor(\n\u001b[1;32m    105\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    106\u001b[0m     prediction_length\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mprediction_length,\n\u001b[1;32m    107\u001b[0m     future_token\u001b[38;5;241m=\u001b[39mfuture_token,\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# 执行模型评估\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_invalid_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan_forecast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseasonality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseason_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# 将结果追加到 CSV 文件\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(csv_file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n",
      "File \u001b[0;32m/home/pai/lib/python3.11/site-packages/gluonts/model/evaluation.py:260\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_data, metrics, axis, batch_size, mask_invalid_label, allow_nan_forecast, seasonality)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(\n\u001b[1;32m    237\u001b[0m     model: Predictor,\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m     seasonality: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    Evaluate ``model`` when applied to ``test_data``, according to ``metrics``.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Return results as a Pandas ``DataFrame``.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     forecasts \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluate_forecasts(\n\u001b[1;32m    263\u001b[0m         forecasts\u001b[38;5;241m=\u001b[39mforecasts,\n\u001b[1;32m    264\u001b[0m         test_data\u001b[38;5;241m=\u001b[39mtest_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m         seasonality\u001b[38;5;241m=\u001b[39mseasonality,\n\u001b[1;32m    271\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[12], line 130\u001b[0m, in \u001b[0;36mYingLongPredictor.predict\u001b[0;34m(self, test_data_input, batch_size)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(batcher(test_data_input, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)):\n\u001b[1;32m    128\u001b[0m     context \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m    129\u001b[0m     forecast_outputs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 130\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfuture_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuture_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpredict_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    136\u001b[0m     )\n\u001b[1;32m    137\u001b[0m forecast_outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(forecast_outputs)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 119\u001b[0m, in \u001b[0;36mYingLongPredictor.model_predict\u001b[0;34m(self, context, prediction_length, future_token, scaling, max_length, *args, **predict_kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     logits \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logits_all[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m logits_all[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mflip(dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    118\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m used\n\u001b[0;32m--> 119\u001b[0m sampleHolder \u001b[38;5;241m=\u001b[39m \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb l c -> b c l\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()[:, :, :prediction_length]\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnan_to_num(sampleHolder)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 依次执行所有任务\n",
    "for idx, task in enumerate(tasks, 1):\n",
    "    print(f\"\\n=== Running Task {idx} ===\")\n",
    "    run_task(\n",
    "        batch_size=task[\"batch_size\"],\n",
    "        model=model,\n",
    "        future_token=future_token,\n",
    "        short_tasks=task[\"short_tasks\"],\n",
    "        long_tasks=task[\"long_tasks\"],\n",
    "        output_dir=output_dir,\n",
    "        dataset_properties_map=dataset_properties_map,\n",
    "        model_name=model_name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901cfbf-e52b-4a0d-9935-5ea11ee9f3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969734a-2ad0-4c23-aaca-1ccd8182da77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
