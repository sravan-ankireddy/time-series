{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart - Run TiRex on GiftEval\n",
    "\n",
    "This notebook shows how to run [TiRex](https://github.com/NX-AI/tirex) on the gift-eval benchmark.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "Before proceeding, ensure you have the following:\n",
    "(Note: You need a Nvidia GPU with [CUDA compute capabality >= 8.0](https://developer.nvidia.com/cuda-gpus))\n",
    "\n",
    "1. **Optional but suggested: Install conda environment specifed in TiRex Repo**\n",
    "\n",
    "```bash\n",
    "git clone github.com/NX-AI/tirex\n",
    "conda env create --file ./tirex/requirements_py26.yaml\n",
    "conda activate tirex\n",
    "```\n",
    "\n",
    "2. **Install TiRex**\n",
    "\n",
    "```bash\n",
    "git clone github.com/NX-AI/tirex  # if not cloned before\n",
    "cd tirex\n",
    "pip install .  # install tirex\n",
    "```\n",
    "\n",
    "3. **Install additional dependecies needed for GiftEval benchmark**\n",
    "\n",
    "```bash\n",
    "pip install gluonts dotenv datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is the data.py file as in GiftEval but fixed so that one can run it with numpy >2.0.0\n",
    "# (Extended/Fixed the frequency alias)\n",
    "#\n",
    "\n",
    "import os\n",
    "import math\n",
    "from functools import cached_property\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Iterator\n",
    "\n",
    "import datasets\n",
    "from dotenv import load_dotenv\n",
    "from gluonts.dataset import DataEntry\n",
    "from gluonts.dataset.common import ProcessDataEntry\n",
    "from gluonts.dataset.split import TestData, TrainingDataset, split\n",
    "from gluonts.itertools import Map\n",
    "from gluonts.time_feature import norm_freq_str\n",
    "from gluonts.transform import Transformation\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "import pyarrow.compute as pc\n",
    "from toolz import compose\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "MAX_WINDOW = 20\n",
    "\n",
    "M4_PRED_LENGTH_MAP = {\n",
    "    \"A\": 6,\n",
    "    \"Q\": 8,\n",
    "    \"M\": 18,\n",
    "    \"W\": 13,\n",
    "    \"D\": 14,\n",
    "    \"H\": 48,\n",
    "    # new version fix:\n",
    "    \"h\": 48,\n",
    "    \"Y\": 6,\n",
    "\n",
    "}\n",
    "\n",
    "PRED_LENGTH_MAP = {\n",
    "    \"M\": 12,\n",
    "    \"W\": 8,\n",
    "    \"D\": 30,\n",
    "    \"H\": 48,\n",
    "    \"T\": 48,\n",
    "    \"S\": 60,\n",
    "    # new version fix:\n",
    "    \"h\": 48,\n",
    "    \"s\": 60,\n",
    "    \"min\": 48,\n",
    "}\n",
    "\n",
    "TFB_PRED_LENGTH_MAP = {\n",
    "    \"A\": 6,\n",
    "    \"H\": 48,\n",
    "    \"Q\": 8,\n",
    "    \"D\": 14,\n",
    "    \"M\": 18,\n",
    "    \"W\": 13,\n",
    "    \"U\": 8,\n",
    "    \"T\": 8,\n",
    "    # new version fix:\n",
    "    \"min\": 8,\n",
    "    \"us\": 8,\n",
    "    \"Y\": 6,\n",
    "    \"h\": 48,\n",
    "}\n",
    "\n",
    "\n",
    "class Term(Enum):\n",
    "    SHORT = \"short\"\n",
    "    MEDIUM = \"medium\"\n",
    "    LONG = \"long\"\n",
    "\n",
    "    @property\n",
    "    def multiplier(self) -> int:\n",
    "        if self == Term.SHORT:\n",
    "            return 1\n",
    "        elif self == Term.MEDIUM:\n",
    "            return 10\n",
    "        elif self == Term.LONG:\n",
    "            return 15\n",
    "\n",
    "\n",
    "def itemize_start(data_entry: DataEntry) -> DataEntry:\n",
    "    data_entry[\"start\"] = data_entry[\"start\"].item()\n",
    "    return data_entry\n",
    "\n",
    "\n",
    "class MultivariateToUnivariate(Transformation):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "\n",
    "    def __call__(\n",
    "        self, data_it: Iterable[DataEntry], is_train: bool = False\n",
    "    ) -> Iterator:\n",
    "        for data_entry in data_it:\n",
    "            item_id = data_entry[\"item_id\"]\n",
    "            val_ls = list(data_entry[self.field])\n",
    "            for id, val in enumerate(val_ls):\n",
    "                univariate_entry = data_entry.copy()\n",
    "                univariate_entry[self.field] = val\n",
    "                univariate_entry[\"item_id\"] = item_id + \"_dim\" + str(id)\n",
    "                yield univariate_entry\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        term: Term | str = Term.SHORT,\n",
    "        to_univariate: bool = False,\n",
    "        storage_env_var: str = \"GIFT_EVAL\",\n",
    "    ):\n",
    "        load_dotenv()\n",
    "        storage_path = Path(os.getenv(storage_env_var))\n",
    "        self.hf_dataset = datasets.load_from_disk(str(storage_path / name)).with_format(\n",
    "            \"numpy\"\n",
    "        )\n",
    "        process = ProcessDataEntry(\n",
    "            self.freq,\n",
    "            one_dim_target=self.target_dim == 1,\n",
    "        )\n",
    "\n",
    "        self.gluonts_dataset = Map(compose(process, itemize_start), self.hf_dataset)\n",
    "        if to_univariate:\n",
    "            self.gluonts_dataset = MultivariateToUnivariate(\"target\").apply(\n",
    "                self.gluonts_dataset\n",
    "            )\n",
    "\n",
    "        self.term = Term(term)\n",
    "        self.name = name\n",
    "\n",
    "    @cached_property\n",
    "    def prediction_length(self) -> int:\n",
    "        freq = norm_freq_str(to_offset(self.freq).name)\n",
    "        if freq.endswith(\"E\"):\n",
    "            freq = freq[:-1]\n",
    "        pred_len = (\n",
    "            M4_PRED_LENGTH_MAP[freq] if \"m4\" in self.name else PRED_LENGTH_MAP[freq]\n",
    "        )\n",
    "        return self.term.multiplier * pred_len\n",
    "\n",
    "    @cached_property\n",
    "    def freq(self) -> str:\n",
    "        return self.hf_dataset[0][\"freq\"]\n",
    "\n",
    "    @cached_property\n",
    "    def target_dim(self) -> int:\n",
    "        return (\n",
    "            target.shape[0]\n",
    "            if len((target := self.hf_dataset[0][\"target\"]).shape) > 1\n",
    "            else 1\n",
    "        )\n",
    "\n",
    "    @cached_property\n",
    "    def past_feat_dynamic_real_dim(self) -> int:\n",
    "        if \"past_feat_dynamic_real\" not in self.hf_dataset[0]:\n",
    "            return 0\n",
    "        elif (\n",
    "            len(\n",
    "                (\n",
    "                    past_feat_dynamic_real := self.hf_dataset[0][\n",
    "                        \"past_feat_dynamic_real\"\n",
    "                    ]\n",
    "                ).shape\n",
    "            )\n",
    "            > 1\n",
    "        ):\n",
    "            return past_feat_dynamic_real.shape[0]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    @cached_property\n",
    "    def windows(self) -> int:\n",
    "        if \"m4\" in self.name:\n",
    "            return 1\n",
    "        w = math.ceil(TEST_SPLIT * self._min_series_length / self.prediction_length)\n",
    "        return min(max(1, w), MAX_WINDOW)\n",
    "\n",
    "    @cached_property\n",
    "    def _min_series_length(self) -> int:\n",
    "        if self.hf_dataset[0][\"target\"].ndim > 1:\n",
    "            lengths = pc.list_value_length(\n",
    "                pc.list_flatten(\n",
    "                    pc.list_slice(self.hf_dataset.data.column(\"target\"), 0, 1)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            lengths = pc.list_value_length(self.hf_dataset.data.column(\"target\"))\n",
    "        return min(lengths.to_numpy())\n",
    "\n",
    "    @cached_property\n",
    "    def sum_series_length(self) -> int:\n",
    "        if self.hf_dataset[0][\"target\"].ndim > 1:\n",
    "            lengths = pc.list_value_length(\n",
    "                pc.list_flatten(self.hf_dataset.data.column(\"target\"))\n",
    "            )\n",
    "        else:\n",
    "            lengths = pc.list_value_length(self.hf_dataset.data.column(\"target\"))\n",
    "        return sum(lengths.to_numpy())\n",
    "\n",
    "    @property\n",
    "    def training_dataset(self) -> TrainingDataset:\n",
    "        training_dataset, _ = split(\n",
    "            self.gluonts_dataset, offset=-self.prediction_length * (self.windows + 1)\n",
    "        )\n",
    "        return training_dataset\n",
    "\n",
    "    @property\n",
    "    def validation_dataset(self) -> TrainingDataset:\n",
    "        validation_dataset, _ = split(\n",
    "            self.gluonts_dataset, offset=-self.prediction_length * self.windows\n",
    "        )\n",
    "        return validation_dataset\n",
    "\n",
    "    @property\n",
    "    def test_data(self) -> TestData:\n",
    "        _, test_template = split(\n",
    "            self.gluonts_dataset, offset=-self.prediction_length * self.windows\n",
    "        )\n",
    "        test_data = test_template.generate_instances(\n",
    "            prediction_length=self.prediction_length,\n",
    "            windows=self.windows,\n",
    "            distance=self.prediction_length,\n",
    "        )\n",
    "        return test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from gluonts.ev.metrics import (\n",
    "    MSE,\n",
    "    MAE,\n",
    "    MASE,\n",
    "    MAPE,\n",
    "    SMAPE,\n",
    "    MSIS,\n",
    "    RMSE,\n",
    "    NRMSE,\n",
    "    ND,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# avoid exessive logging\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")\n",
    "\n",
    "SHORT_DATA = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "MED_LONG_DATA = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "PRETTY_NAMES = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "ALL_DATASETS = list(set(SHORT_DATA.split() + MED_LONG_DATA.split()))\n",
    "\n",
    "METRICS = [\n",
    "        MSE(forecast_type=\"mean\"),\n",
    "        MSE(forecast_type=0.5),\n",
    "        MAE(),\n",
    "        MASE(),\n",
    "        MAPE(),\n",
    "        SMAPE(),\n",
    "        MSIS(),\n",
    "        RMSE(),\n",
    "        NRMSE(),\n",
    "        ND(),\n",
    "        MeanWeightedSumQuantileLoss(\n",
    "            quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "try:\n",
    "    \n",
    "    with open(Path.cwd() / \"dataset_properties.json\", 'r') as f:\n",
    "        dataset_properties_map = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    raise ValueError(\"Can not find needed dataset_properties.json file!\")\n",
    "\n",
    "\n",
    "def gift_eval_dataset_iter():\n",
    "    for ds_num, ds_name in enumerate(ALL_DATASETS):\n",
    "        ds_key = ds_name.split(\"/\")[0]\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        for term in terms:\n",
    "            if (\n",
    "                term == \"medium\" or term == \"long\"\n",
    "            ) and ds_name not in MED_LONG_DATA.split():\n",
    "                continue\n",
    "\n",
    "            if \"/\" in ds_name:\n",
    "                ds_key = ds_name.split(\"/\")[0]\n",
    "                ds_freq = ds_name.split(\"/\")[1]\n",
    "                ds_key = ds_key.lower()\n",
    "                ds_key = PRETTY_NAMES.get(ds_key, ds_key)\n",
    "            else:\n",
    "                ds_key = ds_name.lower()\n",
    "                ds_key = PRETTY_NAMES.get(ds_key, ds_key)\n",
    "                ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "            yield {\"ds_name\": ds_name,\"ds_key\": ds_key,\n",
    "                   \"ds_freq\": ds_freq,\"term\": term}\n",
    "\n",
    "\n",
    "# Setup GiftEval Evalution\n",
    "def evaluate_dataset(predictor, ds_name, ds_key, ds_freq, term):\n",
    "    print(f\"Processing dataset: {ds_name}\")\n",
    "    ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "    # Initialize the dataset\n",
    "    to_univariate = (\n",
    "        False\n",
    "        if Dataset(name=ds_name, term=term, to_univariate=False).target_dim == 1\n",
    "        else True\n",
    "    )\n",
    "    dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "    predictor.set_prediction_len(dataset.prediction_length)\n",
    "    predictor.set_ds_freq(ds_freq)\n",
    "    season_length = get_seasonality(dataset.freq)\n",
    "\n",
    "    # Measure the time taken for evaluation\n",
    "    res = evaluate_model(\n",
    "        predictor,\n",
    "        test_data=dataset.test_data,\n",
    "        metrics=METRICS,\n",
    "        batch_size=1024,\n",
    "        axis=None,\n",
    "        mask_invalid_label=True,\n",
    "        allow_nan_forecast=False,\n",
    "        seasonality=season_length,\n",
    "    )\n",
    "    result = {\n",
    "        \"dataset\": ds_config,\n",
    "        \"model\": predictor.model_id,\n",
    "        \"eval_metrics/MSE[mean]\": res[\"MSE[mean]\"][0],\n",
    "        \"eval_metrics/MSE[0.5]\": res[\"MSE[0.5]\"][0],\n",
    "        \"eval_metrics/MAE[0.5]\": res[\"MAE[0.5]\"][0],\n",
    "        \"eval_metrics/MASE[0.5]\": res[\"MASE[0.5]\"][0],\n",
    "        \"eval_metrics/MAPE[0.5]\": res[\"MAPE[0.5]\"][0],\n",
    "        \"eval_metrics/sMAPE[0.5]\": res[\"sMAPE[0.5]\"][0],\n",
    "        \"eval_metrics/MSIS\": res[\"MSIS\"][0],\n",
    "        \"eval_metrics/RMSE[mean]\": res[\"RMSE[mean]\"][0],\n",
    "        \"eval_metrics/NRMSE[mean]\": res[\"NRMSE[mean]\"][0],\n",
    "        \"eval_metrics/ND[0.5]\": res[\"ND[0.5]\"][0],\n",
    "        \"eval_metrics/mean_weighted_sum_quantile_loss\": res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "        \"domain\": dataset_properties_map[ds_key][\"domain\"],\n",
    "        \"num_variates\": dataset_properties_map[ds_key][\"num_variates\"],\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TiRexGiftEvalWrapper():\n",
    "    model: Any\n",
    "    freq: str = None\n",
    "    pred_len: int = 32\n",
    "\n",
    "    def set_ds_freq(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "    def set_prediction_len(self, pred_len):\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def predict(self, test_data_input):\n",
    "        forecasts = self.model.forecast_gluon(test_data_input, prediction_length=self.pred_len, output_type=\"gluonts\")\n",
    "        return forecasts\n",
    "\n",
    "    @property\n",
    "    def model_id(self):\n",
    "        return \"TiRex\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Runs the Evalutions \n",
    "(make sure to execute the blocks before to init all depdent classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tirex import load_model, ForecastModel\n",
    "\n",
    "#os.environ[\"GIFT-EVAL\"] = \"/path/to/gifteval\" # Set GIFT-EVAL data path if not already specified\n",
    "\n",
    "model : ForecastModel = load_model(\"NX-AI/TiRex\")\n",
    "\n",
    "wrapped_model = TiRexGiftEvalWrapper(model)\n",
    "results = []\n",
    "for task in gift_eval_dataset_iter():\n",
    "    task_result = evaluate_dataset(wrapped_model, **task)\n",
    "    results.append(task_result)\n",
    "    print(task_result)\n",
    "results = pd.DataFrame(results)\n",
    "print(results)\n",
    "results.to_csv(\"./all_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
